{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chun1225/Imperial-Research-Project/blob/main/RP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pyg-lib using the recommended method from the error message\n",
        "# This ensures compatibility with your installed PyTorch and CUDA versions\n",
        "import torch\n",
        "\n",
        "# Detect PyTorch and CUDA version\n",
        "TORCH_VERSION = torch.__version__.split('+')[0]\n",
        "CUDA_VERSION = torch.version.cuda\n",
        "\n",
        "if CUDA_VERSION:\n",
        "    # Format as e.g. 'cu118' or 'cu121'\n",
        "    CUDA_str = f\"cu{CUDA_VERSION.replace('.', '')}\"\n",
        "else:\n",
        "    # If no CUDA, use cpu version\n",
        "    CUDA_str = 'cpu'\n",
        "\n",
        "# Construct and execute the correct installation command\n",
        "# !pip is the syntax to run shell commands in Jupyter/Colab environments\n",
        "!pip install pyg-lib -f https://data.pyg.org/whl/torch-{TORCH_VERSION}+{CUDA_str}.html"
      ],
      "metadata": {
        "id": "zVmEBYJ3H6vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHIgqFT0FyOH"
      },
      "outputs": [],
      "source": [
        "pip install category_encoders catboost dataframe_image Selenium torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJE4m2r-Fs6w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import dataframe_image as dfi\n",
        "from IPython.display import display, Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, average_precision_score, precision_recall_curve\n",
        "\n",
        "import category_encoders as ce\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv, Linear, BatchNorm, GATv2Conv\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMMoP38Jtdzg"
      },
      "outputs": [],
      "source": [
        "TRANSACTIONS_PATH = '/content/drive/MyDrive/credit_card_transactions-ibm_v2.csv'\n",
        "CARDS_INFO_PATH = '/content/drive/MyDrive/sd254_cards.csv'\n",
        "USERS_INFO_PATH = '/content/drive/MyDrive/sd254_users.csv'\n",
        "\n",
        "full_df = pd.read_csv(TRANSACTIONS_PATH)\n",
        "df_trans = full_df.sample(n=1500000, random_state=335)\n",
        "df_cards = pd.read_csv(CARDS_INFO_PATH)\n",
        "df_users = pd.read_csv(USERS_INFO_PATH)\n",
        "\n",
        "# merge\n",
        "\n",
        "df_trans['Amount'] = df_trans['Amount'].replace({'\\$': ''}, regex=True).astype(float)\n",
        "df_trans['Is Fraud?'] = df_trans['Is Fraud?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "df_card_owner_profile = pd.merge(\n",
        "    df_cards,\n",
        "    df_users,\n",
        "    on='User',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "df_full_context = pd.merge(\n",
        "    df_trans,\n",
        "    df_card_owner_profile,\n",
        "    on=['User', 'Card'],\n",
        "    how='left',\n",
        "    suffixes=('_trans', '')\n",
        ")\n",
        "\n",
        "print(\"dim:\", df_full_context.shape)\n",
        "\n",
        "display(df_full_context.head())\n",
        "print(\"Info: \")\n",
        "df_full_context.info(verbose=True, show_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_full_context' not in locals():\n",
        "    print(\"without df_full_context。\")\n",
        "else:\n",
        "    sns.set_style(\"whitegrid\")\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    ax = sns.countplot(x='Is Fraud?', data=df_full_context, palette=['#66b3ff', '#ff9999'])\n",
        "\n",
        "    ax.set_title('Distribution of Fraud vs Non-Fraud Samples', fontsize=16, pad=20)\n",
        "    ax.set_xlabel('Class', fontsize=12)\n",
        "    ax.set_ylabel('Number of Transactions', fontsize=12)\n",
        "\n",
        "    ax.set_xticklabels(['Non-Fraud (0)', 'Fraud (1)'])\n",
        "\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{int(p.get_height()):,}',\n",
        "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center',\n",
        "                    va='center',\n",
        "                    xytext=(0, 9),\n",
        "                    textcoords='offset points',\n",
        "                    fontsize=11)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1cYageVjR-L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMJT-8QT0Zia"
      },
      "outputs": [],
      "source": [
        "def split_composite_errors(error_string: str) -> pd.Series:\n",
        "    error_string = str(error_string).strip()\n",
        "    if ',' in error_string:\n",
        "        parts = error_string.split(',', 1)\n",
        "        return pd.Series([parts[0].strip(), parts[1].strip()])\n",
        "    else:\n",
        "        return pd.Series([error_string, 'None'])\n",
        "\n",
        "def create_final_feature_set(df: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    cols_to_clean_numeric = ['Credit Limit', 'Yearly Income - Person', 'Total Debt', 'Per Capita Income - Zipcode']\n",
        "    for col in cols_to_clean_numeric:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = pd.to_numeric(df_processed[col].astype(str).str.replace(r'[$,]', '', regex=True), errors='coerce')\n",
        "    if 'Amount' in df_processed.columns:\n",
        "        df_processed['Amount'] = df_processed['Amount'].abs()\n",
        "    id_cols_to_str = ['User', 'Merchant Name', 'Zip', 'MCC', 'Card Brand', 'Card Type', 'Use Chip', 'Has Chip', 'State', 'Merchant State', 'Merchant City', 'City', 'Gender', 'Card on Dark Web']\n",
        "    for col in id_cols_to_str:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = df_processed[col].astype(str)\n",
        "    if 'Errors?' in df_processed.columns:\n",
        "        df_processed['Errors?'] = df_processed['Errors?'].fillna('No Error').astype(str)\n",
        "\n",
        "\n",
        "    if 'User' in df_processed.columns and 'Card' in df_processed.columns:\n",
        "        df_processed['Card'] = df_processed['User']+ '_' + df_processed['Card'].astype(str)\n",
        "\n",
        "    if all(c in df_processed.columns for c in ['Year', 'Month', 'Day', 'Time']):\n",
        "        df_processed['trans_datetime'] = pd.to_datetime(df_processed[['Year', 'Month', 'Day']].astype(str).agg('-'.join, axis=1) + ' ' + df_processed['Time'], errors='coerce')\n",
        "        df_processed['day_of_week_sin'] = np.sin(2 * np.pi * df_processed['trans_datetime'].dt.dayofweek / 7.0)\n",
        "        df_processed['day_of_week_cos'] = np.cos(2 * np.pi * df_processed['trans_datetime'].dt.dayofweek / 7.0)\n",
        "        df_processed['hour_sin'] = np.sin(2 * np.pi * df_processed['trans_datetime'].dt.hour / 24.0)\n",
        "        df_processed['hour_cos'] = np.cos(2 * np.pi * df_processed['trans_datetime'].dt.hour / 24.0)\n",
        "        if 'Acct Open Date' in df_processed.columns:\n",
        "            acct_open_year = pd.to_datetime(df_processed['Acct Open Date'], format='%m/%Y', errors='coerce').dt.year\n",
        "            df_processed['account_age_at_transaction'] = df_processed['Year'] - acct_open_year\n",
        "    if 'State' in df_processed.columns and 'Merchant State' in df_processed.columns:\n",
        "        df_processed['is_in_home_state'] = (df_processed['State'] == df_processed['Merchant State']).astype(int)\n",
        "    if 'City' in df_processed.columns and 'Merchant City' in df_processed.columns:\n",
        "        df_processed['is_in_home_city'] = (df_processed['City'] == df_processed['Merchant City']).astype(int)\n",
        "    if 'Amount' in df_processed.columns and 'Credit Limit' in df_processed.columns:\n",
        "        df_processed['amount_to_limit_ratio'] = df_processed['Amount'] / df_processed['Credit Limit'].replace(0, 1e-6)\n",
        "    if 'Amount' in df_processed.columns and 'Yearly Income - Person' in df_processed.columns:\n",
        "        df_processed['amount_to_personal_income_ratio'] = df_processed['Amount'] / df_processed['Yearly Income - Person'].replace(0, 1e-6)\n",
        "    if 'Total Debt' in df_processed.columns and 'Yearly Income - Person' in df_processed.columns:\n",
        "        df_processed['debt_to_income_ratio'] = df_processed['Total Debt'] / df_processed['Yearly Income - Person'].replace(0, 1e-6)\n",
        "    if 'Amount' in df_processed.columns and 'Per Capita Income - Zipcode' in df_processed.columns:\n",
        "        df_processed['amount_to_zip_income_ratio'] = df_processed['Amount'] / df_processed['Per Capita Income - Zipcode'].replace(0, 1e-6)\n",
        "    if 'Errors?' in df_processed.columns:\n",
        "        df_processed[['Error1', 'Error2']] = df_processed['Errors?'].apply(split_composite_errors)\n",
        "\n",
        "\n",
        "    final_features_to_keep = [\n",
        "        'User', 'Card', 'Is Fraud?', 'Amount', 'MCC', 'Card Brand', 'Card Type', 'Has Chip', 'Use Chip',\n",
        "        'Cards Issued', 'Credit Limit', 'Current Age', 'Gender', 'FICO Score',\n",
        "        'Num Credit Cards', 'Yearly Income - Person',\n",
        "        'Merchant Name', 'Merchant State', 'Merchant City', 'State', 'City',\n",
        "        'Total Debt',\n",
        "        'Per Capita Income - Zipcode',\n",
        "        'day_of_week_sin', 'day_of_week_cos', 'hour_sin', 'hour_cos',\n",
        "        'account_age_at_transaction',\n",
        "        'is_in_home_state', 'is_in_home_city',\n",
        "        'amount_to_limit_ratio', 'amount_to_personal_income_ratio',\n",
        "        'debt_to_income_ratio', 'amount_to_zip_income_ratio',\n",
        "        'Error1', 'Error2'\n",
        "    ]\n",
        "\n",
        "    final_cols_exist = [col for col in final_features_to_keep if col in df_processed.columns]\n",
        "    df_final = df_processed[final_cols_exist].copy()\n",
        "\n",
        "    for col in df_final.columns:\n",
        "        if df_final[col].dtype.name in ['object', 'category']:\n",
        "            df_final[col] = df_final[col].fillna('Unknown')\n",
        "        else:\n",
        "            df_final[col] = df_final[col].fillna(df_final[col].median())\n",
        "\n",
        "    print(f\"dim: {df_final.shape}\")\n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS4rqbM4hIZ1"
      },
      "outputs": [],
      "source": [
        "df_final = create_final_feature_set(df_full_context)\n",
        "\n",
        "print(\"info: \")\n",
        "print(df_final.shape)\n",
        "df_final.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5o7C8miiLhi"
      },
      "outputs": [],
      "source": [
        "print(\"Cardinality\")\n",
        "\n",
        "categorical_features = df_final.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\" {len(categorical_features)} Categorical variables：\\n\")\n",
        "\n",
        "for col in categorical_features:\n",
        "    unique_count = df_final[col].nunique()\n",
        "    print(f\"'{col}': {unique_count} unique value\")\n",
        "\n",
        "    if unique_count < 20:\n",
        "        print(df_final[col].value_counts().to_string())\n",
        "        print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj8w-2Pkh2f7"
      },
      "outputs": [],
      "source": [
        "TARGET = 'Is Fraud?'\n",
        "features_df = df_final.drop(columns=[TARGET], errors='ignore')\n",
        "numerical_features = features_df.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = features_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "low_cardinality_features = [col for col in categorical_features if features_df[col].nunique() <= 2]\n",
        "high_cardinality_features = [col for col in categorical_features if features_df[col].nunique() > 2]\n",
        "\n",
        "X = df_final.drop(TARGET, axis=1)\n",
        "y = df_final[TARGET]\n",
        "\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "high_cardinality_encoders = {\n",
        "    \"Ordinal\": ce.OrdinalEncoder(handle_unknown='value', handle_missing='value'),\n",
        "    \"Frequency\" : ce.CountEncoder(normalize=True, handle_unknown='value', handle_missing='value'),\n",
        "    \"WOE\": ce.WOEEncoder(regularization=40, handle_unknown='value', handle_missing='value'),\n",
        "    \"JamesStein\": ce.JamesSteinEncoder(model='binary', handle_unknown='value', handle_missing='value'),\n",
        "    \"MEstimate\": ce.MEstimateEncoder(m=25),\n",
        "}\n",
        "\n",
        "models = {\n",
        "    \"DecisionTree\": (DecisionTreeClassifier, {'random_state': 335, 'class_weight': 'balanced'}),\n",
        "    \"RandomForest\": (RandomForestClassifier, {'random_state': 335, 'n_jobs': -1, 'class_weight': 'balanced'}),\n",
        "    \"XGBoost\": (xgb.XGBClassifier, {'random_state': 335, 'eval_metric': 'aucpr', 'tree_method': 'hist', 'device': 'cuda'}),\n",
        "    \"CatBoost\": (cb.CatBoostClassifier, {'random_state': 335, 'task_type': 'GPU', 'verbose': 0})\n",
        "}\n",
        "\n",
        "one_hot_encoder = ce.OneHotEncoder(handle_unknown='value', handle_missing='value', use_cat_names=True)\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scoring_metrics = {\n",
        "    'pr_auc': 'average_precision',\n",
        "    'f1': 'f1',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall'\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "for encoder_name, encoder in high_cardinality_encoders.items():\n",
        "    for model_name, (model_class, model_params) in models.items():\n",
        "        start_time = time.time()\n",
        "        print(f\"\\n{encoder_name} + {model_name}\")\n",
        "\n",
        "        fold_scores = []\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X, y)):\n",
        "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "            dynamic_scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "            current_model_params = model_params.copy()\n",
        "            if model_name in [\"XGBoost\"]:\n",
        "                current_model_params['scale_pos_weight'] = dynamic_scale_pos_weight\n",
        "            current_model = model_class(**current_model_params)\n",
        "\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', Pipeline([\n",
        "                        ('imputer', SimpleImputer(strategy='median')),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), numerical_features),\n",
        "                    ('cat_low', one_hot_encoder, low_cardinality_features),\n",
        "                    ('cat_high', Pipeline([\n",
        "                        ('encoder', encoder),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), high_cardinality_features),\n",
        "                ],\n",
        "                remainder='passthrough'\n",
        "            )\n",
        "\n",
        "            pipeline = Pipeline(steps=[\n",
        "                ('preprocessor', preprocessor),\n",
        "                            ('classifier', current_model)\n",
        "            ])\n",
        "\n",
        "            X_train_to_fit = X_train\n",
        "            y_train_to_fit = y_train\n",
        "\n",
        "            pipeline.fit(X_train_to_fit, y_train_to_fit)\n",
        "\n",
        "            y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "            precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "            f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls) != 0)\n",
        "            best_f1_idx = np.argmax(f1_scores)\n",
        "\n",
        "            fold_scores.append({\n",
        "                'PR AUC': average_precision_score(y_val, y_pred_proba),\n",
        "                'Best F1-Score': f1_scores[best_f1_idx],\n",
        "                'Best Threshold': thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 1.0,\n",
        "                'Precision at Best F1': precisions[best_f1_idx],\n",
        "                'Recall at Best F1': recalls[best_f1_idx]\n",
        "            })\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        avg_scores = pd.DataFrame(fold_scores).mean().to_dict()\n",
        "\n",
        "        results.append({\n",
        "            \"Encoder\": encoder_name, \"Model\": model_name,\n",
        "            \"PR AUC\": avg_scores['PR AUC'],\n",
        "            \"F1-Score\": avg_scores['Best F1-Score'],\n",
        "            \"Precision\": avg_scores['Precision at Best F1'],\n",
        "            \"Recall\": avg_scores['Recall at Best F1'],\n",
        "            \"Duration (s)\": duration\n",
        "        })\n",
        "        print(f\"time: {duration:.2f} s, F1-Score: {avg_scores['Best F1-Score']:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "results_df.rename(columns={\n",
        "    \"F1-Score\": \"Best F1-Score\",\n",
        "    \"Precision\": \"Precision at Best F1\",\n",
        "    \"Recall\": \"Recall at Best F1\"\n",
        "}, inplace=True)\n",
        "column_order = ['Encoder', 'Model', 'PR AUC', 'Best F1-Score', 'Precision at Best F1', 'Recall at Best F1', 'Duration (s)']\n",
        "results_df_sorted = results_df[column_order].sort_values(by=\"Best F1-Score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nResult: \")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "print(results_df_sorted)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_to_check = \"State\"\n",
        "\n",
        "X = df_final.drop(TARGET, axis=1)\n",
        "y = df_final[TARGET]\n",
        "\n",
        "train_idx, _ = next(iter(cv_strategy.split(X, y)))\n",
        "X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "\n",
        "js_encoder = ce.JamesSteinEncoder(model='binary', handle_unknown='value', handle_missing='value')\n",
        "js_encoded = js_encoder.fit_transform(X_train[[feature_to_check]], y_train)\n",
        "\n",
        "woe_encoder = ce.WOEEncoder(regularization=40, handle_unknown='value', handle_missing='value')\n",
        "woe_encoded = woe_encoder.fit_transform(X_train[[feature_to_check]], y_train)\n",
        "\n",
        "freq_encoder = ce.CountEncoder(normalize=True, handle_unknown='value', handle_missing='value')\n",
        "freq_encoded = freq_encoder.fit_transform(X_train[[feature_to_check]])\n",
        "\n",
        "Me_encoder = ce.MEstimateEncoder(m=25)\n",
        "Me_encoded = Me_encoder.fit_transform(X_train[[feature_to_check]], y_train)\n",
        "\n",
        "Ordinal_encoder = ce.OrdinalEncoder(handle_unknown='value', handle_missing='value')\n",
        "Ordinal_encoded = Ordinal_encoder.fit_transform(X_train[[feature_to_check]])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "js_scaled = scaler.fit_transform(js_encoded[[feature_to_check]].values).flatten()\n",
        "woe_scaled = scaler.fit_transform(woe_encoded[[feature_to_check]].values).flatten()\n",
        "freq_scaled = scaler.fit_transform(freq_encoded[[feature_to_check]].values).flatten()\n",
        "Me_scaled = scaler.fit_transform(Me_encoded[[feature_to_check]].values).flatten()\n",
        "Ordinal_scaled = scaler.fit_transform(Ordinal_encoded[[feature_to_check]].values).flatten()\n",
        "\n",
        "encoders_data = {\n",
        "    'M Estimate': Me_scaled,\n",
        "    'James-Stein': js_scaled,\n",
        "    'WOE': woe_scaled,\n",
        "    'Ordinal': Ordinal_scaled,\n",
        "    'Frequency': freq_scaled\n",
        "}\n",
        "\n",
        "plot_df = pd.DataFrame(encoders_data).melt(var_name='Encoder', value_name='Value')\n"
      ],
      "metadata": {
        "id": "ZQ0fA_kd6-EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
        "\n",
        "g = sns.FacetGrid(plot_df, row=\"Encoder\", hue=\"Encoder\", aspect=9, height=1.5, palette=\"deep\")\n",
        "\n",
        "g.map_dataframe(sns.kdeplot, x=\"Value\", fill=True, alpha=0.8)\n",
        "g.map_dataframe(sns.kdeplot, x=\"Value\", color='black', lw=1)\n",
        "\n",
        "def label(x, color, label):\n",
        "    ax = plt.gca()\n",
        "    ax.text(0, 0.2, label, fontweight=\"bold\", color='black',\n",
        "                ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
        "\n",
        "g.map(label, \"Value\")\n",
        "\n",
        "g.figure.subplots_adjust(hspace=-0.2)\n",
        "\n",
        "g.set_titles(\"\")\n",
        "g.set(yticks=[], ylabel=\"\")\n",
        "g.despine(bottom=True, left=True)\n",
        "\n",
        "plt.xlabel('Density functions of the encoded feature \"State\"', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KYS-oYC2Ga5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feature Important\")\n",
        "\n",
        "encoders_for_importance = {\n",
        "    \"Freq_encoder\" : ce.CountEncoder(normalize=True),\n",
        "    \"OrdinalEncoder\": ce.OrdinalEncoder(),\n",
        "    \"WOEEncoder\": ce.WOEEncoder(),\n",
        "}\n",
        "\n",
        "if 'models_to_test' not in locals():\n",
        "    print(\"without 'models_to_test'\")\n",
        "else:\n",
        "    catboost_model_base = models_to_test['CatBoost']\n",
        "\n",
        "    all_importance_dfs = []\n",
        "\n",
        "    for encoder_name, encoder in encoders_for_importance.items():\n",
        "        print(f\"\\nFeature Important: CatBoost + {encoder_name} {'='*25}\")\n",
        "\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', StandardScaler(), numerical_features),\n",
        "                    ('cat', clone(encoder), categorical_features)\n",
        "                ],\n",
        "                remainder='passthrough'\n",
        "            )),\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value=-1)),\n",
        "            ('classifier', clone(catboost_model_base))\n",
        "        ])\n",
        "\n",
        "        pipeline.fit(X, y)\n",
        "\n",
        "        try:\n",
        "            feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
        "\n",
        "            importances = pipeline.named_steps['classifier'].feature_importances_\n",
        "\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': importances\n",
        "            }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "            importance_df['Encoder'] = encoder_name\n",
        "            all_importance_dfs.append(importance_df)\n",
        "\n",
        "            print(\"Top 20:\")\n",
        "            print(importance_df.head(20).to_string(index=False))\n",
        "\n",
        "\n",
        "if 'all_importance_dfs' in locals() and all_importance_dfs:\n",
        "    combined_importance_df = pd.concat(all_importance_dfs, ignore_index=True)\n",
        "\n",
        "    top_features_df = combined_importance_df.groupby('Encoder').apply(lambda x: x.nlargest(10, 'Importance')).reset_index(drop=True)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.barplot(\n",
        "        data=top_features_df,\n",
        "        x='Importance',\n",
        "        y='Feature',\n",
        "        hue='Encoder',\n",
        "        dodge=False\n",
        "    )\n",
        "    plt.title('Top 10 Feature Importance for CatBoost with Different Encoders', fontsize=16)\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.ylabel('Feature Name', fontsize=12)\n",
        "    plt.legend(title='Encoder')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zB0xy_M3N8yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strategy 1"
      ],
      "metadata": {
        "id": "Jmn7MOff_Kgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVRqNRwiRoUI"
      },
      "outputs": [],
      "source": [
        "TARGET = 'Is Fraud?'\n",
        "features_df = df_final.drop(columns=[TARGET], errors='ignore')\n",
        "numerical_features = features_df.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = features_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "low_cardinality_features = [col for col in categorical_features if features_df[col].nunique() <= 15]\n",
        "high_cardinality_features = [col for col in categorical_features if features_df[col].nunique() > 15]\n",
        "\n",
        "X = df_final.drop(TARGET, axis=1)\n",
        "y = df_final[TARGET]\n",
        "\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "high_cardinality_encoders = {\n",
        "    \"Ordinal\": ce.OrdinalEncoder(handle_unknown='value', handle_missing='value'),\n",
        "    \"Frequency\" : ce.CountEncoder(normalize=True, handle_unknown='value', handle_missing='value'),\n",
        "    \"WOE\": ce.WOEEncoder(regularization=40, handle_unknown='value', handle_missing='value'),\n",
        "    \"JamesStein\": ce.JamesSteinEncoder(model='binary', handle_unknown='value', handle_missing='value'),\n",
        "    \"MEstimate\": ce.MEstimateEncoder(m=25),\n",
        "}\n",
        "\n",
        "models = {\n",
        "    \"XGBoost\": (xgb.XGBClassifier, {'random_state': 335, 'eval_metric': 'aucpr', 'tree_method': 'hist', 'device': 'cuda'}),\n",
        "    \"CatBoost\": (cb.CatBoostClassifier, {'random_state': 335, 'task_type': 'GPU', 'verbose': 0})\n",
        "}\n",
        "\n",
        "one_hot_encoder = ce.OneHotEncoder(handle_unknown='value', handle_missing='value', use_cat_names=True)\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scoring_metrics = {\n",
        "    'pr_auc': 'average_precision',\n",
        "    'f1': 'f1',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall'\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for encoder_name, encoder in high_cardinality_encoders.items():\n",
        "    for model_name, (model_class, model_params) in models.items():\n",
        "        start_time = time.time()\n",
        "        print(f\"\\n{encoder_name} + {model_name}...\")\n",
        "\n",
        "        fold_scores = []\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X, y)):\n",
        "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "            dynamic_scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "            current_model_params = model_params.copy()\n",
        "            if model_name in [\"XGBoost\"]:\n",
        "                current_model_params['scale_pos_weight'] = dynamic_scale_pos_weight\n",
        "            current_model = model_class(**current_model_params)\n",
        "\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', Pipeline([\n",
        "                        ('imputer', SimpleImputer(strategy='median')),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), numerical_features),\n",
        "                    ('cat_low', one_hot_encoder, low_cardinality_features),\n",
        "                    ('cat_high', Pipeline([\n",
        "                        ('encoder', encoder),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), high_cardinality_features),\n",
        "                ],\n",
        "                remainder='passthrough'\n",
        "            )\n",
        "\n",
        "            pipeline = Pipeline(steps=[\n",
        "                ('preprocessor', preprocessor),\n",
        "                            ('classifier', current_model)\n",
        "            ])\n",
        "\n",
        "            X_train_to_fit = X_train\n",
        "            y_train_to_fit = y_train\n",
        "\n",
        "            pipeline.fit(X_train_to_fit, y_train_to_fit)\n",
        "\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "            precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "            f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls) != 0)\n",
        "            best_f1_idx = np.argmax(f1_scores)\n",
        "\n",
        "            fold_scores.append({\n",
        "                'PR AUC': average_precision_score(y_val, y_pred_proba),\n",
        "                'Best F1-Score': f1_scores[best_f1_idx],\n",
        "                'Best Threshold': thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 1.0,\n",
        "                'Precision at Best F1': precisions[best_f1_idx],\n",
        "                'Recall at Best F1': recalls[best_f1_idx]\n",
        "            })\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        avg_scores = pd.DataFrame(fold_scores).mean().to_dict()\n",
        "\n",
        "        results.append({\n",
        "            \"Encoder\": encoder_name, \"Model\": model_name,\n",
        "            \"PR AUC\": avg_scores['PR AUC'],\n",
        "            \"F1-Score\": avg_scores['Best F1-Score'],\n",
        "            \"Precision\": avg_scores['Precision at Best F1'],\n",
        "            \"Recall\": avg_scores['Recall at Best F1'],\n",
        "            \"Duration (s)\": duration\n",
        "        })\n",
        "        print(f\"time: {duration:.2f} s, F1-Score: {avg_scores['Best F1-Score']:.4f}\")\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.rename(columns={\n",
        "    \"F1-Score\": \"Best F1-Score\",\n",
        "    \"Precision\": \"Precision at Best F1\",\n",
        "    \"Recall\": \"Recall at Best F1\"\n",
        "}, inplace=True)\n",
        "column_order = ['Encoder', 'Model', 'PR AUC', 'Best F1-Score', 'Precision at Best F1', 'Recall at Best F1', 'Duration (s)']\n",
        "results_df_sorted = results_df[column_order].sort_values(by=\"Best F1-Score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "print(results_df_sorted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strategy 2"
      ],
      "metadata": {
        "id": "N_lT4qpL_PE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_encoders_to_test = {\n",
        "    \"WOE_plus_Frequency\": FeatureUnion([\n",
        "        ('woe_pipeline', Pipeline([('encoder', ce.WOEEncoder(regularization=40))])),\n",
        "        ('freq_pipeline', Pipeline([('encoder', ce.CountEncoder(normalize=True))]))\n",
        "    ]),\n",
        "    \"JS_plus_Frequency\": FeatureUnion([\n",
        "        ('js_pipeline', Pipeline([('encoder', ce.JamesSteinEncoder(model='binary'))])),\n",
        "        ('freq_pipeline', Pipeline([('encoder', ce.CountEncoder(normalize=True))]))\n",
        "    ]),\n",
        "    \"WOE_plus_Ordinal\": FeatureUnion([\n",
        "        ('woE_pipeline', Pipeline([('encoder', ce.WOEEncoder(regularization=40))])),\n",
        "        ('ord_pipeline', Pipeline([('encoder', ce.OrdinalEncoder())]))\n",
        "    ]),\n",
        "    \"JS_plus_Ordinal\": FeatureUnion([\n",
        "        ('js_pipeline', Pipeline([('encoder', ce.JamesSteinEncoder(model='binary'))])),\n",
        "        ('ord_pipeline', Pipeline([('encoder', ce.OrdinalEncoder())]))\n",
        "    ])\n",
        "}\n",
        "\n",
        "models_to_test = {\n",
        "    \"XGBoost\": (xgb.XGBClassifier, {'random_state': 335, 'eval_metric': 'aucpr', 'tree_method': 'hist', 'device': 'cuda'}),\n",
        "    \"CatBoost\": (cb.CatBoostClassifier, {'random_state': 335, 'task_type': 'GPU', 'verbose': 0})\n",
        "}\n",
        "\n",
        "all_results = []\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=335)\n",
        "one_hot_encoder = ce.OneHotEncoder(handle_unknown='value', handle_missing='value', use_cat_names=True)\n",
        "\n",
        "for encoder_name, combined_encoder in combined_encoders_to_test.items():\n",
        "    for model_name, (model_class, model_params) in models_to_test.items():\n",
        "        print(f\"\\n{encoder_name} + {model_name} {'='*25}\")\n",
        "\n",
        "        fold_results = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X, y)):\n",
        "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "            dynamic_scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "            current_model_params = model_params.copy()\n",
        "            if model_name == \"XGBoost\":\n",
        "                current_model_params['scale_pos_weight'] = dynamic_scale_pos_weight\n",
        "\n",
        "            model = model_class(**current_model_params)\n",
        "\n",
        "            # FeatureUnion\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', Pipeline([\n",
        "                        ('imputer', SimpleImputer(strategy='median')),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), numerical_features),\n",
        "\n",
        "                    ('cat_low', one_hot_encoder, low_cardinality_features),\n",
        "\n",
        "                    ('cat_high_combined', combined_encoder, high_cardinality_features)\n",
        "                ],\n",
        "                remainder='passthrough'\n",
        "            )\n",
        "\n",
        "            pipeline = Pipeline(steps=[\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('classifier', model)\n",
        "            ])\n",
        "\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "\n",
        "            precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "            f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls) != 0)\n",
        "            best_f1_idx = np.argmax(f1_scores)\n",
        "\n",
        "            fold_results.append({\n",
        "                \"PR AUC\": average_precision_score(y_val, y_pred_proba),\n",
        "                \"Best F1-Score\": f1_scores[best_f1_idx],\n",
        "                \"Precision at Best F1\": precisions[best_f1_idx],\n",
        "                \"Recall at Best F1\": recalls[best_f1_idx]\n",
        "            })\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        avg_results = pd.DataFrame(fold_results).mean().to_dict()\n",
        "        avg_results['Encoder_Combination'] = encoder_name\n",
        "        avg_results['Model'] = model_name\n",
        "        avg_results['Duration_s'] = duration\n",
        "        all_results.append(avg_results)\n",
        "        print(f\"time: {duration:.2f} s, PR AUC: {avg_results['PR AUC']:.4f}, F1-Score: {avg_results['Best F1-Score']:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\nResult: \")\n",
        "results_df = pd.DataFrame(all_results)\n",
        "column_order = ['Encoder_Combination', 'Model', 'PR AUC', 'Best F1-Score', 'Precision at Best F1', 'Recall at Best F1', 'Duration_s']\n",
        "results_df_sorted = results_df[column_order].sort_values(by=\"PR AUC\", ascending=False)\n",
        "print(results_df_sorted.to_string(index=False))"
      ],
      "metadata": {
        "id": "srLLQ3VH_IO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_composite_errors(error_string: str) -> pd.Series:\n",
        "    error_string = str(error_string).strip()\n",
        "    if ',' in error_string:\n",
        "        parts = error_string.split(',', 1)\n",
        "        return pd.Series([parts[0].strip(), parts[1].strip()])\n",
        "    else:\n",
        "        return pd.Series([error_string, 'None'])\n",
        "\n",
        "def create_final_feature_set_gnn(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    cols_to_clean_numeric = ['Credit Limit', 'Yearly Income - Person', 'Total Debt', 'Per Capita Income - Zipcode']\n",
        "    for col in cols_to_clean_numeric:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = pd.to_numeric(df_processed[col].astype(str).str.replace(r'[$,]', '', regex=True), errors='coerce')\n",
        "    if 'Amount' in df_processed.columns:\n",
        "        df_processed['Amount'] = df_processed['Amount'].abs()\n",
        "    id_cols_to_str = ['User', 'Merchant Name', 'Zip', 'MCC', 'Card Brand', 'Card Type', 'Use Chip', 'Has Chip', 'State', 'Merchant State', 'Merchant City', 'City', 'Gender', 'Card on Dark Web']\n",
        "    for col in id_cols_to_str:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = df_processed[col].astype(str)\n",
        "    if 'Errors?' in df_processed.columns:\n",
        "        df_processed['Errors?'] = df_processed['Errors?'].fillna('No Error').astype(str)\n",
        "\n",
        "    if all(c in df_processed.columns for c in ['Year', 'Month', 'Day', 'Time']):\n",
        "        df_processed['trans_datetime'] = pd.to_datetime(df_processed[['Year', 'Month', 'Day']].astype(str).agg('-'.join, axis=1) + ' ' + df_processed['Time'], errors='coerce')\n",
        "        df_processed['day_of_week_sin'] = np.sin(2 * np.pi * df_processed['trans_datetime'].dt.dayofweek / 7.0)\n",
        "        df_processed['day_of_week_cos'] = np.cos(2 * np.pi * df_processed['trans_datetime'].dt.dayofweek / 7.0)\n",
        "        df_processed['hour_sin'] = np.sin(2 * np.pi * df_processed['trans_datetime'].dt.hour / 24.0)\n",
        "        df_processed['hour_cos'] = np.cos(2 * np.pi * df_processed['trans_datetime'].dt.hour / 24.0)\n",
        "        if 'Acct Open Date' in df_processed.columns:\n",
        "            acct_open_year = pd.to_datetime(df_processed['Acct Open Date'], format='%m/%Y', errors='coerce').dt.year\n",
        "            df_processed['account_age_at_transaction'] = df_processed['Year'] - acct_open_year\n",
        "    if 'State' in df_processed.columns and 'Merchant State' in df_processed.columns:\n",
        "        df_processed['is_in_home_state'] = (df_processed['State'] == df_processed['Merchant State']).astype(int)\n",
        "    if 'City' in df_processed.columns and 'Merchant City' in df_processed.columns:\n",
        "        df_processed['is_in_home_city'] = (df_processed['City'] == df_processed['Merchant City']).astype(int)\n",
        "    if 'Amount' in df_processed.columns and 'Credit Limit' in df_processed.columns:\n",
        "        df_processed['amount_to_limit_ratio'] = df_processed['Amount'] / df_processed['Credit Limit'].replace(0, 1e-6)\n",
        "    if 'Amount' in df_processed.columns and 'Yearly Income - Person' in df_processed.columns:\n",
        "        df_processed['amount_to_personal_income_ratio'] = df_processed['Amount'] / df_processed['Yearly Income - Person'].replace(0, 1e-6)\n",
        "    if 'Total Debt' in df_processed.columns and 'Yearly Income - Person' in df_processed.columns:\n",
        "        df_processed['debt_to_income_ratio'] = df_processed['Total Debt'] / df_processed['Yearly Income - Person'].replace(0, 1e-6)\n",
        "    if 'Amount' in df_processed.columns and 'Per Capita Income - Zipcode' in df_processed.columns:\n",
        "        df_processed['amount_to_zip_income_ratio'] = df_processed['Amount'] / df_processed['Per Capita Income - Zipcode'].replace(0, 1e-6)\n",
        "    if 'Errors?' in df_processed.columns:\n",
        "        df_processed[['Error1_cat', 'Error2_cat']] = df_processed['Errors?'].apply(split_composite_errors)\n",
        "\n",
        "\n",
        "    final_features_to_keep = [\n",
        "        'Is Fraud?', 'User', 'Card', 'Amount', 'MCC', 'Card Brand', 'Card Type', 'Has Chip', 'Use Chip',\n",
        "        'Cards Issued', 'Credit Limit', 'Current Age', 'Gender', 'FICO Score',\n",
        "        'Num Credit Cards', 'Yearly Income - Person',\n",
        "        'Merchant Name', 'Merchant State', 'Merchant City', 'State', 'City',\n",
        "        'Total Debt',\n",
        "        'Per Capita Income - Zipcode',\n",
        "        'day_of_week_sin', 'day_of_week_cos', 'hour_sin', 'hour_cos',\n",
        "        'account_age_at_transaction',\n",
        "        'is_in_home_state', 'is_in_home_city',\n",
        "        'amount_to_limit_ratio', 'amount_to_personal_income_ratio',\n",
        "        'debt_to_income_ratio', 'amount_to_zip_income_ratio',\n",
        "        'Error1_cat', 'Error2_cat'\n",
        "    ]\n",
        "\n",
        "    final_cols_exist = [col for col in final_features_to_keep if col in df_processed.columns]\n",
        "    df_final = df_processed[final_cols_exist].copy()\n",
        "\n",
        "    for col in df_final.columns:\n",
        "        if df_final[col].dtype.name in ['object', 'category']:\n",
        "            df_final[col] = df_final[col].fillna('Unknown')\n",
        "        else:\n",
        "            df_final[col] = df_final[col].fillna(df_final[col].median())\n",
        "\n",
        "    print(f\"dim: {df_final.shape}\")\n",
        "    return df_final"
      ],
      "metadata": {
        "id": "8hXfSUbVyd9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = create_final_feature_set_gnn(df_full_context)\n",
        "\n",
        "print(\"dim:\", df_final.shape)\n",
        "df_final.info()"
      ],
      "metadata": {
        "id": "gzLaptDiy_Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ID\n",
        "df_final['user_id'], user_uniques = pd.factorize(df_final['User'])\n",
        "df_final['card_id'], card_uniques = pd.factorize(df_final['User'].astype(str) + '_' + df_final['Card'].astype(str))\n",
        "df_final['merchant_id'], merchant_uniques = pd.factorize(df_final['Merchant Name'])\n",
        "\n",
        "# City, State, MCC\n",
        "all_cities = pd.concat([df_final['City'], df_final['Merchant City']]).unique()\n",
        "all_states = pd.concat([df_final['State'], df_final['Merchant State']]).unique()\n",
        "all_mccs = df_final['MCC'].unique()\n",
        "\n",
        "city_map = {city: i for i, city in enumerate(all_cities)}\n",
        "state_map = {state: i for i, state in enumerate(all_states)}\n",
        "mcc_map = {mcc: i for i, mcc in enumerate(all_mccs)}\n",
        "\n",
        "df_final['user_city_id'] = df_final['City'].map(city_map)\n",
        "df_final['merchant_city_id'] = df_final['Merchant City'].map(city_map)\n",
        "df_final['user_state_id'] = df_final['State'].map(state_map)\n",
        "df_final['merchant_state_id'] = df_final['Merchant State'].map(state_map)\n",
        "df_final['mcc_id'] = df_final['MCC'].map(mcc_map)\n",
        "\n",
        "\n",
        "print(\"\\nHeteroData\")\n",
        "graph_data = HeteroData()\n",
        "\n",
        "graph_data['user'].num_nodes = len(user_uniques)\n",
        "graph_data['card'].num_nodes = len(card_uniques)\n",
        "graph_data['merchant'].num_nodes = len(merchant_uniques)\n",
        "graph_data['city'].num_nodes = len(all_cities)\n",
        "graph_data['state'].num_nodes = len(all_states)\n",
        "graph_data['mcc'].num_nodes = len(all_mccs)\n",
        "\n",
        "# user\n",
        "user_num_features = ['Current Age', 'FICO Score', 'Num Credit Cards', 'Yearly Income - Person', 'Total Debt']\n",
        "user_cat_features = ['Gender']\n",
        "user_features_df = df_final.groupby('user_id')[user_num_features + user_cat_features].first()\n",
        "user_preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), user_num_features),\n",
        "    ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), user_cat_features)\n",
        "])\n",
        "processed_user_features = user_preprocessor.fit_transform(user_features_df)\n",
        "graph_data['user'].x = torch.tensor(processed_user_features, dtype=torch.float)\n",
        "\n",
        "# card\n",
        "card_num_features = ['Credit Limit', 'Cards Issued']\n",
        "card_cat_features = ['Card Brand', 'Card Type', 'Has Chip']\n",
        "card_features_df = df_final.groupby('card_id')[card_num_features + card_cat_features].first()\n",
        "card_preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), card_num_features),\n",
        "    ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), card_cat_features)\n",
        "])\n",
        "processed_card_features = card_preprocessor.fit_transform(card_features_df)\n",
        "graph_data['card'].x = torch.tensor(processed_card_features, dtype=torch.float)\n",
        "\n",
        "\n",
        "print(\"\\nEdge Index\")\n",
        "\n",
        "# user-card\n",
        "user_card_edges = df_final[['user_id', 'card_id']].drop_duplicates()\n",
        "edge_index_user_owns_card = torch.tensor(user_card_edges.values.T, dtype=torch.long)\n",
        "graph_data['user', 'owns', 'card'].edge_index = edge_index_user_owns_card\n",
        "graph_data['card', 'owned_by', 'user'].edge_index = edge_index_user_owns_card[[1, 0]]\n",
        "\n",
        "# card-merchant\n",
        "card_merchant_edges = df_final[['card_id', 'merchant_id']]\n",
        "edge_index_card_interacts_merchant = torch.tensor(card_merchant_edges.values.T, dtype=torch.long)\n",
        "graph_data['card', 'interacts_with', 'merchant'].edge_index = edge_index_card_interacts_merchant\n",
        "graph_data['merchant', 'rev_interacts_with', 'card'].edge_index = edge_index_card_interacts_merchant[[1, 0]]\n",
        "\n",
        "# user-city\n",
        "user_city_edges = df_final[['user_id', 'user_city_id']].drop_duplicates()\n",
        "edge_index_user_lives_in_city = torch.tensor(user_city_edges.values.T, dtype=torch.long)\n",
        "graph_data['user', 'lives_in', 'city'].edge_index = edge_index_user_lives_in_city\n",
        "graph_data['city', 'hosts_user', 'user'].edge_index = edge_index_user_lives_in_city[[1, 0]]\n",
        "\n",
        "# merchant-city\n",
        "merchant_city_edges = df_final[['merchant_id', 'merchant_city_id']].drop_duplicates()\n",
        "edge_index_merchant_in_city = torch.tensor(merchant_city_edges.values.T, dtype=torch.long)\n",
        "graph_data['merchant', 'located_in', 'city'].edge_index = edge_index_merchant_in_city\n",
        "graph_data['city', 'hosts_merchant', 'merchant'].edge_index = edge_index_merchant_in_city[[1, 0]]\n",
        "\n",
        "# city-state\n",
        "city_state_map_df = pd.concat([\n",
        "    df_final[['user_city_id', 'user_state_id']].rename(columns={'user_city_id': 'city_id', 'user_state_id': 'state_id'}),\n",
        "    df_final[['merchant_city_id', 'merchant_state_id']].rename(columns={'merchant_city_id': 'city_id', 'merchant_state_id': 'state_id'})\n",
        "]).drop_duplicates().dropna()\n",
        "edge_index_city_in_state = torch.tensor(city_state_map_df.values.T, dtype=torch.long)\n",
        "graph_data['city', 'part_of', 'state'].edge_index = edge_index_city_in_state\n",
        "graph_data['state', 'contains', 'city'].edge_index = edge_index_city_in_state[[1, 0]]\n",
        "\n",
        "# merchant-MCC\n",
        "merchant_mcc_edges = df_final[['merchant_id', 'mcc_id']].drop_duplicates()\n",
        "edge_index_merchant_has_mcc = torch.tensor(merchant_mcc_edges.values.T, dtype=torch.long)\n",
        "graph_data['merchant', 'has_category', 'mcc'].edge_index = edge_index_merchant_has_mcc\n",
        "graph_data['mcc', 'is_category_of', 'merchant'].edge_index = edge_index_merchant_has_mcc[[1, 0]]\n",
        "\n",
        "# Transaction edge\n",
        "\n",
        "edge_numeric_cols = ['Amount', 'day_of_week_sin', 'day_of_week_cos', 'hour_sin', 'hour_cos']\n",
        "edge_categorical_cols = ['Use Chip', 'Error1_cat', 'Error2_cat']\n",
        "\n",
        "edge_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), edge_numeric_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), edge_categorical_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "edge_feature_source_cols = edge_numeric_cols + edge_categorical_cols\n",
        "processed_edge_features = edge_preprocessor.fit_transform(df_final[edge_feature_source_cols])\n",
        "\n",
        "# Transaction edge card-merchant\n",
        "graph_data['card', 'interacts_with', 'merchant'].edge_attr = torch.tensor(processed_edge_features, dtype=torch.float)\n",
        "\n",
        "print(f\"dim: {processed_edge_features.shape[1]}\")\n",
        "\n",
        "print(\"Structure\")\n",
        "print(graph_data)"
      ],
      "metadata": {
        "id": "Y27pASVgzugD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HeteroGNN_Refactored(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels, node_types, edge_types, node_feature_dims, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = torch.nn.ModuleDict()\n",
        "        self.linears = torch.nn.ModuleDict()\n",
        "        self.batch_norms = torch.nn.ModuleDict()\n",
        "        for node_type in node_types:\n",
        "            if node_type not in node_feature_dims:\n",
        "                num_nodes = graph_data[node_type].num_nodes\n",
        "                self.embeddings[node_type] = torch.nn.Embedding(num_nodes, hidden_channels)\n",
        "            self.batch_norms[node_type] = BatchNorm(hidden_channels)\n",
        "\n",
        "        for node_type, dim in node_feature_dims.items():\n",
        "            self.linears[node_type] = Linear(dim, hidden_channels)\n",
        "\n",
        "        self.conv1 = HeteroConv({\n",
        "            edge_type: SAGEConv((-1, -1), hidden_channels) for edge_type in edge_types\n",
        "        }, aggr='sum')\n",
        "\n",
        "        self.conv2 = HeteroConv({\n",
        "            edge_type: SAGEConv((-1, -1), out_channels) for edge_type in edge_types\n",
        "        }, aggr='sum')\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "\n",
        "        x_dict_processed = {}\n",
        "        for node_type, x in x_dict.items():\n",
        "            if node_type in self.linears:\n",
        "                x = self.linears[node_type](x)\n",
        "            else:\n",
        "                x = self.embeddings[node_type](x)\n",
        "            x = self.batch_norms[node_type](x).relu()\n",
        "            x_dict_processed[node_type] = x\n",
        "\n",
        "        x_dict = self.conv1(x_dict_processed, edge_index_dict)\n",
        "        x_dict = {key: self.dropout(x.relu()) for key, x in x_dict.items()}\n",
        "\n",
        "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
        "\n",
        "        return x_dict\n",
        "\n",
        "# GNN\n",
        "train_graph = graph_data\n",
        "\n",
        "node_feature_dims = {}\n",
        "for node_type in graph_data.node_types:\n",
        "    if hasattr(graph_data[node_type], 'x') and graph_data[node_type].x is not None:\n",
        "        node_feature_dims[node_type] = graph_data[node_type].x.shape[1]\n",
        "\n",
        "gnn_model = HeteroGNN_Refactored(\n",
        "    hidden_channels=256,\n",
        "    out_channels=128,\n",
        "    node_types=graph_data.node_types,\n",
        "    edge_types=train_graph.edge_types,\n",
        "    node_feature_dims=node_feature_dims,\n",
        "    dropout_rate=0.5\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "gnn_model = gnn_model.to(device)\n",
        "\n",
        "print(gnn_model)\n",
        "\n",
        "# train\n",
        "\n",
        "edge_label_index = train_graph['card', 'interacts_with', 'merchant'].edge_index\n",
        "train_loader = LinkNeighborLoader(\n",
        "    data=train_graph,\n",
        "    num_neighbors=[20, 10],\n",
        "    edge_label_index=(('card', 'interacts_with', 'merchant'), edge_label_index),\n",
        "    neg_sampling_ratio=3.0,\n",
        "    batch_size=2048,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, min_lr=0.00001)\n",
        "\n",
        "# training\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "    gnn_model.train()\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch:02d}'):\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_dict_input = {\n",
        "            node_type: batch[node_type].x if hasattr(batch[node_type], 'x') else batch[node_type].n_id\n",
        "            for node_type in batch.node_types\n",
        "        }\n",
        "        z_dict = gnn_model(x_dict_input, batch.edge_index_dict)\n",
        "\n",
        "        edge_label_index = batch['card', 'interacts_with', 'merchant'].edge_label_index\n",
        "        edge_label = batch['card', 'interacts_with', 'merchant'].edge_label\n",
        "\n",
        "        src_emb = z_dict['card'][edge_label_index[0]]\n",
        "        dst_emb = z_dict['merchant'][edge_label_index[1]]\n",
        "\n",
        "        pred = (src_emb * dst_emb).sum(dim=-1)\n",
        "        loss = F.binary_cross_entropy_with_logits(pred, edge_label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss) * pred.numel()\n",
        "        total_examples += pred.numel()\n",
        "\n",
        "    avg_loss = total_loss / total_examples\n",
        "    scheduler.step(avg_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch}, Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "total_duration = time.time() - start_time\n",
        "print(f\"\\nTime: {total_duration:.2f} s\")"
      ],
      "metadata": {
        "id": "B-N92n8aGXlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnn_model.eval()\n",
        "gnn_model = gnn_model.to('cpu')\n",
        "graph_data = graph_data.to('cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "    full_x_dict_input = {\n",
        "        node_type: graph_data[node_type].x if hasattr(graph_data[node_type], 'x') else torch.arange(graph_data[node_type].num_nodes)\n",
        "        for node_type in graph_data.node_types\n",
        "    }\n",
        "    final_embeddings = gnn_model(full_x_dict_input, graph_data.edge_index_dict)\n",
        "\n",
        "user_embeddings = final_embeddings['user'].numpy()\n",
        "merchant_embeddings = final_embeddings['merchant'].numpy()\n",
        "\n",
        "user_emb_df = pd.DataFrame(user_embeddings, columns=[f'user_emb_{i}' for i in range(user_embeddings.shape[1])])\n",
        "user_emb_df['User'] = user_uniques\n",
        "merchant_emb_df = pd.DataFrame(merchant_embeddings, columns=[f'merchant_emb_{i}' for i in range(merchant_embeddings.shape[1])])\n",
        "merchant_emb_df['Merchant Name'] = merchant_uniques\n",
        "\n",
        "df_augmented = pd.merge(df_final, user_emb_df, on='User', how='left')\n",
        "df_augmented = pd.merge(df_augmented, merchant_emb_df, on='Merchant Name', how='left')\n",
        "print(f\"Dim: {df_augmented.shape}\")\n",
        "\n",
        "TARGET = 'Is Fraud?'\n",
        "cols_to_drop = [TARGET, 'User', 'Card', 'Merchant Name', 'MCC', 'Errors?', 'Card Brand', 'Card Type', 'Has Chip', 'Use Chip', 'Gender', 'Merchant State', 'Merchant City', 'State', 'City', 'Error1_cat', 'Error2_cat']\n",
        "cols_to_drop_exist = [col for col in cols_to_drop if col in df_augmented.columns]\n",
        "X_aug = df_augmented.drop(columns=cols_to_drop_exist)\n",
        "y_aug = df_augmented[TARGET]\n",
        "\n",
        "models_to_test = {\n",
        "    \"DecisionTree_GNN\": (DecisionTreeClassifier, {'random_state': 335, 'class_weight': 'balanced'}),\n",
        "    \"RandomForest_GNN\": (RandomForestClassifier, {'random_state': 335, 'n_jobs': -1, 'class_weight': 'balanced'}),\n",
        "    \"XGBoost_GNN\": (xgb.XGBClassifier, {'random_state': 335, 'eval_metric': 'aucpr', 'tree_method': 'hist', 'device': 'cuda'}),\n",
        "    \"CatBoost_GNN\": (cb.CatBoostClassifier, {'random_state': 335, 'task_type': 'GPU',  'verbose': 0}),\n",
        "}\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "all_model_results = {}\n",
        "\n",
        "for model_name, (model_class, model_params) in models_to_test.items():\n",
        "    print(f\"\\n{model_name}\")\n",
        "    all_fold_results = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(tqdm(cv_strategy.split(X_aug, y_aug), total=5, desc=f\"({model_name})\")):\n",
        "        X_train, X_val = X_aug.iloc[train_idx], X_aug.iloc[val_idx]\n",
        "        y_train, y_val = y_aug.iloc[train_idx], y_aug.iloc[val_idx]\n",
        "\n",
        "        dynamic_scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "        current_model_params = model_params.copy()\n",
        "        if model_name in [\"XGBoost_GNN\"]:\n",
        "            current_model_params['scale_pos_weight'] = dynamic_scale_pos_weight\n",
        "\n",
        "        current_model = model_class(**current_model_params)\n",
        "\n",
        "        pipeline = Pipeline(steps=[('scaler', StandardScaler()), ('classifier', current_model)])\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "        precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "        fscores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls) != 0)\n",
        "        ix = np.argmax(fscores)\n",
        "        best_threshold = thresholds[ix] if ix < len(thresholds) else 1.0\n",
        "        y_pred_class_optimal = (y_pred_proba >= best_threshold).astype(int)\n",
        "\n",
        "        fold_scores = {\n",
        "            'PR AUC': average_precision_score(y_val, y_pred_proba),\n",
        "            'F1-Score': f1_score(y_val, y_pred_class_optimal, zero_division=0),\n",
        "            'Precision': precision_score(y_val, y_pred_class_optimal, zero_division=0),\n",
        "            'Recall': recall_score(y_val, y_pred_class_optimal, zero_division=0),\n",
        "            'Optimal Threshold': best_threshold\n",
        "        }\n",
        "        all_fold_results.append(fold_scores)\n",
        "\n",
        "    results_df = pd.DataFrame(all_fold_results)\n",
        "    all_model_results[model_name] = results_df.agg(['mean', 'std'])\n",
        "\n",
        "\n",
        "for model_name, summary_df in all_model_results.items():\n",
        "    print(f\"\\n{model_name}\")\n",
        "    print(summary_df)"
      ],
      "metadata": {
        "id": "8Nv2oK1VcndZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1V5v7sX_4qQhZyE5UfNRF52xpNTzoViUk",
      "authorship_tag": "ABX9TyNcEI6NMRi9mb2Qso0pUjCw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}