{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chun1225/Imperial-Research-Project/blob/main/RP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT9g1cun80DI"
      },
      "outputs": [],
      "source": [
        "# Install pyg-lib using the recommended method from the error message\n",
        "# This ensures compatibility with your installed PyTorch and CUDA versions\n",
        "import torch\n",
        "\n",
        "# Detect PyTorch and CUDA version\n",
        "TORCH_VERSION = torch.__version__.split('+')[0]\n",
        "CUDA_VERSION = torch.version.cuda\n",
        "\n",
        "if CUDA_VERSION:\n",
        "    # Format as e.g. 'cu118' or 'cu121'\n",
        "    CUDA_str = f\"cu{CUDA_VERSION.replace('.', '')}\"\n",
        "else:\n",
        "    # If no CUDA, use cpu version\n",
        "    CUDA_str = 'cpu'\n",
        "\n",
        "\n",
        "# Construct and execute the correct installation command\n",
        "# !pip is the syntax to run shell commands in Jupyter/Colab environments\n",
        "!pip install pyg-lib -f https://data.pyg.org/whl/torch-{TORCH_VERSION}+{CUDA_str}.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXf5VYZCa-PC"
      },
      "outputs": [],
      "source": [
        "pip install category_encoders catboost dataframe_image Selenium torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvYzomLEa_9R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import dataframe_image as dfi\n",
        "from IPython.display import display, Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, average_precision_score, precision_recall_curve\n",
        "\n",
        "import category_encoders as ce\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv, HeteroConv, Linear, BatchNorm\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "from tqdm.auto import tqdm\n",
        "from torch_geometric.nn import GATv2Conv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGAYGus4COXX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/fraudTrain.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"can't find 'fraud.csv'\")\n",
        "    exit()\n",
        "\n",
        "print(\"info\")\n",
        "print(f\"dim: {df.shape}\")\n",
        "display(df.head())\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng-gUs2sFQvB"
      },
      "outputs": [],
      "source": [
        "if 'Unnamed: 0' in df.columns:\n",
        "    df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# datetime\n",
        "df['trans_datetime'] = pd.to_datetime(df['trans_date_trans_time'])\n",
        "df['dob_datetime'] = pd.to_datetime(df['dob'])\n",
        "\n",
        "df['trans_datetime'] = pd.to_datetime(df['trans_date_trans_time'])\n",
        "df['dob_datetime'] = pd.to_datetime(df['dob'])\n",
        "\n",
        "df['age'] = (df['trans_datetime'] - df['dob_datetime']).dt.days / 365.25\n",
        "df['age'] = df['age'].astype(int)\n",
        "\n",
        "# hour\n",
        "hour_in_day = 24\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['trans_datetime'].dt.hour / hour_in_day)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['trans_datetime'].dt.hour / hour_in_day)\n",
        "df['hour'] = df['trans_datetime'].dt.hour\n",
        "\n",
        "# day of week\n",
        "day_in_week = 7\n",
        "df['day_of_week_sin'] = np.sin(2 * np.pi * df['trans_datetime'].dt.dayofweek / day_in_week)\n",
        "df['day_of_week_cos'] = np.cos(2 * np.pi * df['trans_datetime'].dt.dayofweek / day_in_week)\n",
        "df['day_of_week'] = df['trans_datetime'].dt.dayofweek.astype(str)\n",
        "\n",
        "# distance\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    R = 6371\n",
        "\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_km'] = haversine_distance(\n",
        "    df['lat'], df['long'],\n",
        "    df['merch_lat'], df['merch_long']\n",
        ")\n",
        "\n",
        "cols_to_drop = ['trans_date_trans_time', 'dob', 'trans_datetime', 'dob_datetime']\n",
        "df_processed = df.drop(columns=cols_to_drop)\n",
        "df_processed['cc_num'] = df_processed['cc_num'].astype(str)\n",
        "\n",
        "cols_to_drop_for_baseline = ['first', 'last', 'street', 'trans_num', 'hour', 'day_of_week', 'lat', 'long', 'zip', 'merch_lat', 'merch_long']\n",
        "df_processed = df_processed.drop(columns=cols_to_drop_for_baseline, errors='ignore')\n",
        "\n",
        "print(f\"dum: {df_processed.shape}\")\n",
        "display(df_processed.head())\n",
        "df_processed.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl0FRPKuN86C"
      },
      "outputs": [],
      "source": [
        "fraud_distribution = df_processed['is_fraud'].value_counts(normalize=True) * 100\n",
        "print(f\"0: {fraud_distribution[0]:.4f}%\")\n",
        "print(f\"1:   {fraud_distribution[1]:.4f}%\")\n",
        "\n",
        "\n",
        "# Cardinality\n",
        "\n",
        "categorical_features = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "cardinality_data = []\n",
        "for col in categorical_features:\n",
        "    cardinality = df_processed[col].nunique()\n",
        "    cardinality_data.append([col, cardinality])\n",
        "\n",
        "cardinality_df = pd.DataFrame(cardinality_data, columns=['Feature', 'Cardinality'])\n",
        "cardinality_df = cardinality_df.sort_values(by='Cardinality', ascending=False)\n",
        "\n",
        "print(cardinality_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-K3WF4plFrY"
      },
      "outputs": [],
      "source": [
        "TARGET = 'is_fraud'\n",
        "X = df_processed.drop(columns=[TARGET])\n",
        "y = df_processed[TARGET]\n",
        "\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "low_cardinality_features = [c for c in categorical_features if X[c].nunique(dropna=False) <= 2]\n",
        "high_cardinality_features = [c for c in categorical_features if X[c].nunique(dropna=False) > 2]\n",
        "\n",
        "encoders_to_test = {\n",
        "    \"Ordinal\": ce.OrdinalEncoder(),\n",
        "    \"Frequency\": ce.CountEncoder(normalize=True),\n",
        "    \"WOE\": ce.WOEEncoder(regularization=60),\n",
        "    \"JamesStein\": ce.JamesSteinEncoder(model='binary'),\n",
        "    \"MEstimate\": ce.MEstimateEncoder(m=25),\n",
        "}\n",
        "\n",
        "one_hot_low = ce.OneHotEncoder(handle_missing='value', handle_unknown='value', use_cat_names=True)\n",
        "\n",
        "models = {\n",
        "    \"DecisionTree\": (DecisionTreeClassifier, {'random_state': 335, 'class_weight': 'balanced'}),\n",
        "    \"RandomForest\": (RandomForestClassifier, {'random_state': 335, 'n_jobs': -1, 'min_samples_leaf': 3}),\n",
        "    \"XGBoost\": (xgb.XGBClassifier, {'random_state': 335, 'eval_metric': 'aucpr', 'tree_method': 'hist', 'device': 'cuda'}),\n",
        "    \"CatBoost\": (cb.CatBoostClassifier, {'random_state': 335, 'task_type': 'GPU', 'verbose': 0})\n",
        "}\n",
        "\n",
        "all_folds_list = []\n",
        "all_results = []\n",
        "trained_pipelines = {}\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=335)\n",
        "\n",
        "for encoder_name, encoder in encoders_to_test.items():\n",
        "    for model_name, (model_class, model_params) in models.items():\n",
        "        print(f\"\\n{encoder_name} + {model_name}\")\n",
        "\n",
        "        fold_results = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X, y)):\n",
        "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "            current_model_params = model_params.copy()\n",
        "\n",
        "            model = model_class(**current_model_params)\n",
        "\n",
        "            # Pipeline\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', Pipeline([\n",
        "                        ('imputer', SimpleImputer(strategy='median')),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), numerical_features),\n",
        "                    ('cat_low', one_hot_low, low_cardinality_features),\n",
        "                    ('cat_high', Pipeline([\n",
        "                        ('encoder', encoder),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), high_cardinality_features),\n",
        "                ],\n",
        "                remainder='passthrough'\n",
        "            )\n",
        "\n",
        "            pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                       ('classifier', model)])\n",
        "\n",
        "            X_train_to_fit = X_train\n",
        "            y_train_to_fit = y_train\n",
        "\n",
        "            pipeline.fit(X_train_to_fit, y_train_to_fit)\n",
        "\n",
        "            if fold == 4:\n",
        "                trained_pipelines[f\"{encoder_name} + {model_name}\"] = pipeline\n",
        "            y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "            precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "            f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls) != 0)\n",
        "            best_f1_score = np.max(f1_scores)\n",
        "            best_f1_idx = np.argmax(f1_scores)\n",
        "\n",
        "            all_folds_list.append({\n",
        "                \"Model\": model_name,\n",
        "                \"Encoder\": encoder_name,\n",
        "                \"Fold\": fold,\n",
        "                \"F1-Score\": best_f1_score\n",
        "            })\n",
        "\n",
        "            fold_results.append({\n",
        "                \"PR AUC\": average_precision_score(y_val, y_pred_proba),\n",
        "                \"Best F1-Score\": f1_scores[best_f1_idx],\n",
        "                \"Best Threshold\": thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 1.0,\n",
        "                \"Precision at Best F1\": precisions[best_f1_idx],\n",
        "                \"Recall at Best F1\": recalls[best_f1_idx]\n",
        "            })\n",
        "        duration = time.time() - start_time\n",
        "        fold_results_df = pd.DataFrame(fold_results)\n",
        "        avg_results = fold_results_df.mean().to_dict()\n",
        "        avg_results['Encoder'] = encoder_name\n",
        "        avg_results['Model'] = model_name\n",
        "        avg_results['Duration (s)'] = duration\n",
        "        all_results.append(avg_results)\n",
        "        print(f\"time: {duration:.2f} s, Avg F1-Score: {avg_results['Best F1-Score']:.4f}, Avg PR AUC: {avg_results['PR AUC']:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "column_order = ['Encoder', 'Model', 'PR AUC', 'Best F1-Score', 'Precision at Best F1', 'Recall at Best F1', 'Best Threshold', 'Duration (s)']\n",
        "results_df_sorted = results_df[column_order].sort_values(by=\"Best F1-Score\", ascending=False)\n",
        "\n",
        "print(\"Result:\")\n",
        "print(results_df_sorted.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = 'is_fraud'\n",
        "X = df_processed.drop(columns=[TARGET])\n",
        "y = df_processed[TARGET]\n",
        "\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# <15\n",
        "low_cardinality_features = [c for c in categorical_features if X[c].nunique(dropna=False) <= 15]\n",
        "high_cardinality_features = [c for c in categorical_features if X[c].nunique(dropna=False) > 15]\n",
        "\n",
        "encoders_to_test = {\n",
        "    \"Ordinal\": ce.OrdinalEncoder(),\n",
        "    \"Frequency\": ce.CountEncoder(normalize=True),\n",
        "    \"WOE\": ce.WOEEncoder(regularization=60),\n",
        "    \"JamesStein\": ce.JamesSteinEncoder(model='binary'),\n",
        "    \"MEstimate\": ce.MEstimateEncoder(m=25),\n",
        "}\n",
        "\n",
        "one_hot_low = ce.OneHotEncoder(handle_missing='value', handle_unknown='value', use_cat_names=True)\n",
        "\n",
        "models = {\n",
        "    \"XGBoost\": (xgb.XGBClassifier, {\n",
        "        'random_state': 335,\n",
        "        'eval_metric': 'aucpr',\n",
        "        'tree_method': 'hist',\n",
        "        'device': 'cuda'\n",
        "    }),\n",
        "    \"CatBoost\": (cb.CatBoostClassifier, {'random_state': 335, 'task_type': 'GPU', 'verbose': 0})\n",
        "}\n",
        "\n",
        "all_folds_list = []\n",
        "all_results = []\n",
        "trained_pipelines = {}\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=335)\n",
        "\n",
        "for encoder_name, encoder in encoders_to_test.items():\n",
        "    for model_name, (model_class, model_params) in models.items():\n",
        "        print(f\"\\n{encoder_name} + {model_name}\")\n",
        "\n",
        "        fold_results = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X, y)):\n",
        "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "            current_model_params = model_params.copy()\n",
        "\n",
        "            model = model_class(**current_model_params)\n",
        "\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', Pipeline([\n",
        "                        ('imputer', SimpleImputer(strategy='median')),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), numerical_features),\n",
        "                    ('cat_low', one_hot_low, low_cardinality_features),\n",
        "                    ('cat_high', Pipeline([\n",
        "                        ('encoder', encoder),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), high_cardinality_features),\n",
        "                ],\n",
        "                remainder='passthrough'\n",
        "            )\n",
        "\n",
        "            pipeline = Pipeline(steps=[\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('classifier', model)\n",
        "            ])\n",
        "\n",
        "            X_train_to_fit = X_train\n",
        "            y_train_to_fit = y_train\n",
        "\n",
        "            pipeline.fit(X_train_to_fit, y_train_to_fit)\n",
        "\n",
        "            if fold == 4:\n",
        "                trained_pipelines[f\"{encoder_name} + {model_name}\"] = pipeline\n",
        "\n",
        "            y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "            precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "            f1_scores = np.divide(2 * precisions * recalls, precisions + recalls,\n",
        "                                  out=np.zeros_like(precisions), where=(precisions + recalls) != 0)\n",
        "            best_f1_idx = np.argmax(f1_scores)\n",
        "            best_f1_score = f1_scores[best_f1_idx]\n",
        "            pr_auc = average_precision_score(y_val, y_pred_proba)\n",
        "\n",
        "            best_thr = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 1.0\n",
        "            prec_best = precisions[best_f1_idx]\n",
        "            rec_best = recalls[best_f1_idx]\n",
        "\n",
        "            all_folds_list.append({\n",
        "                \"Model\": model_name,\n",
        "                \"Encoder\": encoder_name,\n",
        "                \"Fold\": fold,\n",
        "                \"PR AUC\": pr_auc,\n",
        "                \"F1-Score\": best_f1_score,\n",
        "                \"Precision\": prec_best,\n",
        "                \"Recall\": rec_best,\n",
        "                \"Best Threshold\": best_thr\n",
        "            })\n",
        "\n",
        "            fold_results.append({\n",
        "                \"PR AUC\": pr_auc,\n",
        "                \"Best F1-Score\": best_f1_score,\n",
        "                \"Precision at Best F1\": prec_best,\n",
        "                \"Recall at Best F1\": rec_best,\n",
        "                \"Best Threshold\": best_thr\n",
        "            })\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        fold_results_df = pd.DataFrame(fold_results)\n",
        "        avg_results = fold_results_df.mean().to_dict()\n",
        "        avg_results['Encoder'] = encoder_name\n",
        "        avg_results['Model'] = model_name\n",
        "        avg_results['Duration (s)'] = duration\n",
        "        all_results.append(avg_results)\n",
        "\n",
        "        print(f\"time: {duration:.2f} s,  Avg F1-Score: {avg_results['Best F1-Score']:.4f}, Avg PR AUC: {avg_results['PR AUC']:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "column_order = ['Encoder', 'Model', 'PR AUC', 'Best F1-Score',\n",
        "                'Precision at Best F1', 'Recall at Best F1',\n",
        "                'Best Threshold', 'Duration (s)']\n",
        "results_df_sorted = results_df[column_order].sort_values(by=\"Best F1-Score\", ascending=False)\n",
        "\n",
        "print(\"Result:\")\n",
        "print(results_df_sorted.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "-udj8FZIhkCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_encoders_to_test = {\n",
        "    \"WOE_plus_Frequency\": FeatureUnion([\n",
        "        ('woe_pipeline', Pipeline([('encoder', ce.WOEEncoder(regularization=60))])),\n",
        "        ('freq_pipeline', Pipeline([('encoder', ce.CountEncoder(normalize=True))]))\n",
        "    ]),\n",
        "    \"JS_plus_Frequency\": FeatureUnion([\n",
        "        ('js_pipeline', Pipeline([('encoder', ce.JamesSteinEncoder(model='binary'))])),\n",
        "        ('freq_pipeline', Pipeline([('encoder', ce.CountEncoder(normalize=True))]))\n",
        "    ]),\n",
        "    \"WOE_plus_JS\": FeatureUnion([\n",
        "        ('woe_pipeline', Pipeline([('encoder', ce.WOEEncoder(regularization=60))])),\n",
        "        ('js_pipeline', Pipeline([('encoder', ce.JamesSteinEncoder(model='binary'))]))\n",
        "    ]),\n",
        "    \"JS_plus_Ordinal\": FeatureUnion([\n",
        "        ('js_pipeline', Pipeline([('encoder', ce.JamesSteinEncoder(model='binary'))])),\n",
        "        ('ord_pipeline', Pipeline([('encoder', ce.OrdinalEncoder())]))\n",
        "    ])\n",
        "}\n",
        "\n",
        "models_to_test = {\n",
        "    \"XGBoost\": xgb.XGBClassifier(random_state=335, eval_metric='aucpr', tree_method='hist', device='cuda'),\n",
        "    \"CatBoost\": cb.CatBoostClassifier(random_state=335, verbose=0, task_type='GPU')\n",
        "}\n",
        "\n",
        "all_results = []\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=335)\n",
        "\n",
        "for encoder_name, combined_encoder in combined_encoders_to_test.items():\n",
        "    for model_name, model_instance in models_to_test.items():\n",
        "        print(f\"\\n{encoder_name} + {model_name}\")\n",
        "\n",
        "        fold_results = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X, y)):\n",
        "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', Pipeline([\n",
        "                        ('imputer', SimpleImputer(strategy='median')),\n",
        "                        ('scaler', StandardScaler())\n",
        "                    ]), numerical_features),\n",
        "\n",
        "                    ('cat_low', ce.OneHotEncoder(handle_missing='value', handle_unknown='value', use_cat_names=True), low_cardinality_features),\n",
        "\n",
        "                    ('cat_high_combined', combined_encoder, high_cardinality_features)\n",
        "                ],\n",
        "                remainder='passthrough'\n",
        "            )\n",
        "\n",
        "            final_pipeline = Pipeline(steps=[\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('classifier', model_instance)\n",
        "            ])\n",
        "\n",
        "            final_pipeline.fit(X_train, y_train)\n",
        "            y_pred_proba = final_pipeline.predict_proba(X_val)[:, 1]\n",
        "\n",
        "            pr_auc = average_precision_score(y_val, y_pred_proba)\n",
        "\n",
        "            precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "            f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls) != 0)\n",
        "            best_f1_idx = np.argmax(f1_scores)\n",
        "\n",
        "            fold_results.append({\n",
        "                \"PR AUC\": pr_auc,\n",
        "                \"Best F1-Score\": f1_scores[best_f1_idx],\n",
        "                \"Precision at Best F1\": precisions[best_f1_idx],\n",
        "                \"Recall at Best F1\": recalls[best_f1_idx]\n",
        "            })\n",
        "\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        fold_results_df = pd.DataFrame(fold_results)\n",
        "        avg_results = fold_results_df.mean().to_dict()\n",
        "        avg_results['Encoder_Combination'] = encoder_name\n",
        "        avg_results['Model'] = model_name\n",
        "        avg_results['Duration_s'] = duration\n",
        "        all_results.append(avg_results)\n",
        "        print(f\"time: {duration:.2f} s, Avg PR AUC: {avg_results['PR AUC']:.4f}, Avg F1-Score: {avg_results['Best F1-Score']:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "column_order = [\n",
        "    'Encoder_Combination', 'Model', 'PR AUC', 'Best F1-Score',\n",
        "    'Precision at Best F1', 'Recall at Best F1', 'Duration_s'\n",
        "]\n",
        "results_df_sorted = results_df[column_order].sort_values(by=\"PR AUC\", ascending=False)\n",
        "\n",
        "print(\"Result:\")\n",
        "print(results_df_sorted.to_string(index=False))"
      ],
      "metadata": {
        "id": "ObH_RUcPDeMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'trained_pipelines' not in locals() or not trained_pipelines:\n",
        "    print(\"can't find 'trained_pipelines'\")\n",
        "\n",
        "else:\n",
        "    for combination_name, pipeline in trained_pipelines.items():\n",
        "        print(f\"\\nFeature important: {combination_name}\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            trained_model = pipeline.named_steps['classifier']\n",
        "            fitted_preprocessor = pipeline.named_steps['preprocessor']\n",
        "\n",
        "            if not hasattr(trained_model, 'feature_importances_'):\n",
        "                continue\n",
        "\n",
        "            feature_names_out = fitted_preprocessor.get_feature_names_out()\n",
        "\n",
        "            importances = trained_model.feature_importances_\n",
        "\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': feature_names_out,\n",
        "                'Importance': importances\n",
        "            }).sort_values(by='Importance', ascending=False).head(20)\n",
        "\n",
        "            print(importance_df.to_string(index=False))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n"
      ],
      "metadata": {
        "id": "-Nd_oH5RhlVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdxwOEvPwH3S"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/fraudTrain.csv')\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df = df.drop(columns=['Unnamed: 0'])\n",
        "    print(f\"dim: {df.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"can't find\")\n",
        "    exit()\n",
        "\n",
        "df['trans_datetime'] = pd.to_datetime(df['trans_date_trans_time'])\n",
        "df['dob_datetime'] = pd.to_datetime(df['dob'])\n",
        "df['age'] = ((df['trans_datetime'] - df['dob_datetime']).dt.days / 365.25).astype(int)\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    R = 6371\n",
        "    lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    return R * c\n",
        "\n",
        "df['distance_km'] = haversine_distance(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
        "\n",
        "# ID\n",
        "customer_codes, customer_uniques = pd.factorize(df['cc_num'])\n",
        "df['customer_id'] = customer_codes\n",
        "customer_mapping = {cc_num: i for i, cc_num in enumerate(customer_uniques)}\n",
        "\n",
        "merchant_codes, merchant_uniques = pd.factorize(df['merchant'])\n",
        "df['merchant_id'] = merchant_codes\n",
        "merchant_mapping = {merchant: i for i, merchant in enumerate(merchant_uniques)}\n",
        "\n",
        "job_codes, job_uniques = pd.factorize(df['job'])\n",
        "df['job_id'] = job_codes\n",
        "job_mapping = {job: i for i, job in enumerate(job_uniques)}\n",
        "\n",
        "category_codes, category_uniques = pd.factorize(df['category'])\n",
        "df['category_id'] = category_codes\n",
        "category_mapping = {category: i for i, category in enumerate(category_uniques)}\n",
        "\n",
        "city_codes, city_uniques = pd.factorize(df['city'])\n",
        "df['city_id'] = city_codes\n",
        "city_mapping = {city: i for i, city in enumerate(city_uniques)}\n",
        "\n",
        "state_codes, state_uniques = pd.factorize(df['state'])\n",
        "df['state_id'] = state_codes\n",
        "state_mapping = {state: i for i, state in enumerate(state_uniques)}\n",
        "\n",
        "# HeteroData\n",
        "graph_data = HeteroData()\n",
        "\n",
        "graph_data['customer'].num_nodes = df['customer_id'].max() + 1\n",
        "graph_data['merchant'].num_nodes = df['merchant_id'].max() + 1\n",
        "graph_data['job'].num_nodes = df['job_id'].max() + 1\n",
        "graph_data['category'].num_nodes = df['category_id'].max() + 1\n",
        "graph_data['city'].num_nodes = df['city_id'].max() + 1\n",
        "graph_data['state'].num_nodes = df['state_id'].max() + 1\n",
        "\n",
        "customer_num_features = ['age', 'lat', 'long', 'city_pop']\n",
        "customer_cat_features = ['gender']\n",
        "customer_features_df = df.drop_duplicates(subset=['customer_id']).set_index('customer_id').sort_index()\n",
        "customer_preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), customer_num_features),\n",
        "    ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), customer_cat_features)\n",
        "])\n",
        "processed_customer_features = customer_preprocessor.fit_transform(customer_features_df[customer_num_features + customer_cat_features])\n",
        "graph_data['customer'].x = torch.tensor(processed_customer_features, dtype=torch.float)\n",
        "\n",
        "merchant_num_features = ['merch_lat', 'merch_long']\n",
        "merchant_features_df = df.drop_duplicates(subset=['merchant_id']).set_index('merchant_id').sort_index()\n",
        "processed_merchant_features = StandardScaler().fit_transform(merchant_features_df[merchant_num_features])\n",
        "graph_data['merchant'].x = torch.tensor(processed_merchant_features, dtype=torch.float)\n",
        "\n",
        "# Edge\n",
        "\n",
        "edge_type = ('customer', 'performs_transaction', 'merchant')\n",
        "edges = df[['customer_id', 'merchant_id']].values.T\n",
        "edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "graph_data[edge_type].edge_index = edge_index\n",
        "rev_edge_type = (edge_type[2], f\"rev_{edge_type[1]}\", edge_type[0])\n",
        "graph_data[rev_edge_type].edge_index = edge_index[[1, 0]]\n",
        "print(f\"transaction edge: {edge_type} ({edge_index.shape[1]})\")\n",
        "\n",
        "structural_edge_definitions = {\n",
        "    ('merchant', 'has_category', 'category'): ['merchant_id', 'category_id'],\n",
        "    ('customer', 'has_job', 'job'): ['customer_id', 'job_id'],\n",
        "    ('customer', 'lives_in', 'city'): ['customer_id', 'city_id'],\n",
        "    ('city', 'is_in_state', 'state'): ['city_id', 'state_id'],\n",
        "    ('merchant', 'located_in', 'city'): ['merchant_id', 'city_id']\n",
        "}\n",
        "for edge_type, (src_col, dst_col) in structural_edge_definitions.items():\n",
        "    edges = df[[src_col, dst_col]].drop_duplicates().values.T\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "    graph_data[edge_type].edge_index = edge_index\n",
        "    rev_edge_type = (edge_type[2], f\"rev_{edge_type[1]}\", edge_type[0])\n",
        "    graph_data[rev_edge_type].edge_index = edge_index[[1, 0]]\n",
        "\n",
        "\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['trans_datetime'].dt.hour / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['trans_datetime'].dt.hour / 24)\n",
        "df['day_of_week_sin'] = np.sin(2 * np.pi * df['trans_datetime'].dt.dayofweek / 7)\n",
        "df['day_of_week_cos'] = np.cos(2 * np.pi * df['trans_datetime'].dt.dayofweek / 7)\n",
        "\n",
        "edge_feature_cols = [\n",
        "    'amt',\n",
        "    'distance_km', # 將 distance_km 加入邊的特徵\n",
        "    'hour_sin', 'hour_cos',\n",
        "    'day_of_week_sin', 'day_of_week_cos'\n",
        "]\n",
        "edge_preprocessor = StandardScaler()\n",
        "processed_edge_features = edge_preprocessor.fit_transform(df[edge_feature_cols])\n",
        "\n",
        "graph_data['customer', 'performs_transaction', 'merchant'].edge_attr = torch.tensor(processed_edge_features, dtype=torch.float)\n",
        "\n",
        "print(f\"dim: {processed_edge_features.shape[1]}\")\n",
        "\n",
        "print(\"Structure\")\n",
        "print(graph_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqFQMgun8Myc"
      },
      "outputs": [],
      "source": [
        "# GNN\n",
        "\n",
        "class HeteroGNN_Optimized(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels, node_types, edge_types, node_feature_dims, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = torch.nn.ModuleDict()\n",
        "        self.linears = torch.nn.ModuleDict()\n",
        "        self.batch_norms = torch.nn.ModuleDict()\n",
        "        for node_type in node_types:\n",
        "            if node_type not in node_feature_dims:\n",
        "                num_nodes = graph_data[node_type].num_nodes\n",
        "                self.embeddings[node_type] = torch.nn.Embedding(num_nodes, hidden_channels)\n",
        "            self.batch_norms[node_type] = BatchNorm(hidden_channels)\n",
        "\n",
        "        for node_type, dim in node_feature_dims.items():\n",
        "            self.linears[node_type] = Linear(dim, hidden_channels)\n",
        "\n",
        "        self.conv1 = HeteroConv({\n",
        "            edge_type: SAGEConv((-1, -1), hidden_channels) for edge_type in edge_types\n",
        "        }, aggr='sum')\n",
        "\n",
        "        self.conv2 = HeteroConv({\n",
        "            edge_type: SAGEConv((-1, -1), out_channels) for edge_type in edge_types\n",
        "        }, aggr='sum')\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        x_dict_processed = {}\n",
        "        for node_type, x in x_dict.items():\n",
        "            if node_type in self.linears:\n",
        "                x = self.linears[node_type](x)\n",
        "            else:\n",
        "                x = self.embeddings[node_type](x)\n",
        "            x = self.batch_norms[node_type](x).relu()\n",
        "            x_dict_processed[node_type] = x\n",
        "\n",
        "        x_dict = self.conv1(x_dict_processed, edge_index_dict)\n",
        "        x_dict = {key: self.dropout(x.relu()) for key, x in x_dict.items()}\n",
        "\n",
        "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
        "\n",
        "        return x_dict\n",
        "\n",
        "\n",
        "# Train\n",
        "\n",
        "train_graph = graph_data\n",
        "node_feature_dims = {}\n",
        "for node_type in graph_data.node_types:\n",
        "    if hasattr(graph_data[node_type], 'x') and graph_data[node_type].x is not None:\n",
        "        node_feature_dims[node_type] = graph_data[node_type].x.shape[1]\n",
        "\n",
        "gnn_model = HeteroGNN_Optimized(\n",
        "    hidden_channels=256,\n",
        "    out_channels=128,\n",
        "    node_types=graph_data.node_types,\n",
        "    edge_types=train_graph.edge_types,\n",
        "    node_feature_dims=node_feature_dims,\n",
        "    dropout_rate=0.5\n",
        ")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "gnn_model = gnn_model.to(device)\n",
        "\n",
        "\n",
        "edge_label_index = train_graph['customer', 'performs_transaction', 'merchant'].edge_index\n",
        "train_loader = LinkNeighborLoader(\n",
        "    data=train_graph,\n",
        "    num_neighbors=[20, 10],\n",
        "    edge_label_index=(('customer', 'performs_transaction', 'merchant'), edge_label_index),\n",
        "    neg_sampling_ratio=5.0,\n",
        "    batch_size=2048,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, min_lr=0.00001)\n",
        "\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "    gnn_model.train()\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch:02d}'):\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_dict_input = {\n",
        "            node_type: batch[node_type].x if hasattr(batch[node_type], 'x') else batch[node_type].n_id\n",
        "            for node_type in batch.node_types\n",
        "        }\n",
        "        z_dict = gnn_model(x_dict_input, batch.edge_index_dict)\n",
        "\n",
        "        edge_label_index = batch['customer', 'performs_transaction', 'merchant'].edge_label_index\n",
        "        edge_label = batch['customer', 'performs_transaction', 'merchant'].edge_label\n",
        "        src_emb = z_dict['customer'][edge_label_index[0]]\n",
        "        dst_emb = z_dict['merchant'][edge_label_index[1]]\n",
        "        pred = (src_emb * dst_emb).sum(dim=-1)\n",
        "        loss = F.binary_cross_entropy_with_logits(pred, edge_label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * pred.numel()\n",
        "        total_examples += pred.numel()\n",
        "\n",
        "    avg_loss = total_loss / total_examples\n",
        "\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch}, Avg Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5iiZ7uf-RuO"
      },
      "outputs": [],
      "source": [
        "gnn_model.eval()\n",
        "gnn_model = gnn_model.to('cpu')\n",
        "graph_data = graph_data.to('cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "    full_x_dict_input = {\n",
        "        node_type: (\n",
        "            graph_data[node_type].x\n",
        "            if hasattr(graph_data[node_type], 'x')\n",
        "            else torch.arange(graph_data[node_type].num_nodes)\n",
        "        )\n",
        "        for node_type in graph_data.node_types\n",
        "    }\n",
        "    final_embeddings = gnn_model(full_x_dict_input, graph_data.edge_index_dict)\n",
        "\n",
        "customer_embeddings = final_embeddings['customer'].cpu().numpy()\n",
        "merchant_embeddings = final_embeddings['merchant'].cpu().numpy()\n",
        "\n",
        "rev_customer_mapping = {v: k for k, v in customer_mapping.items()}\n",
        "rev_merchant_mapping = {v: k for k, v in merchant_mapping.items()}\n",
        "\n",
        "customer_emb_df = pd.DataFrame(customer_embeddings, columns=[f'c_emb_{i}' for i in range(customer_embeddings.shape[1])])\n",
        "customer_emb_df['cc_num'] = customer_emb_df.index.map(rev_customer_mapping)\n",
        "\n",
        "merchant_emb_df = pd.DataFrame(merchant_embeddings, columns=[f'm_emb_{i}' for i in range(merchant_embeddings.shape[1])])\n",
        "merchant_emb_df['merchant'] = merchant_emb_df.index.map(rev_merchant_mapping)\n",
        "\n",
        "df_augmented = pd.merge(df, customer_emb_df, on='cc_num', how='left')\n",
        "df_augmented = pd.merge(df_augmented, merchant_emb_df, on='merchant', how='left')\n",
        "print(f\"dim: {df_augmented.shape}\")\n",
        "\n",
        "TARGET = 'is_fraud'\n",
        "features_to_drop = [\n",
        "    TARGET, 'trans_date_trans_time', 'dob', 'trans_datetime', 'dob_datetime',\n",
        "    'cc_num', 'first', 'last', 'street', 'trans_num', 'merchant',\n",
        "    'category', 'gender', 'job', 'city', 'state',\n",
        "]\n",
        "features_to_drop.extend([col for col in df_augmented.columns if '_id' in col])\n",
        "\n",
        "X_aug = df_augmented.drop(columns=features_to_drop, errors='ignore')\n",
        "y_aug = df_augmented[TARGET]\n",
        "X_aug.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in X_aug.columns]\n",
        "\n",
        "\n",
        "models_to_test = {\n",
        "    \"DecisionTree\": (DecisionTreeClassifier, {'random_state': 335, 'class_weight': 'balanced'}),\n",
        "    \"RandomForest\": (RandomForestClassifier, {'random_state': 335, 'n_jobs': -1, 'min_samples_leaf': 3}),\n",
        "    \"XGBoost\": (xgb.XGBClassifier, {'random_state': 335, 'eval_metric': 'aucpr', 'tree_method': 'hist', 'device': 'cuda'}),\n",
        "    \"CatBoost\": (cb.CatBoostClassifier, {'random_state': 335, 'task_type': 'GPU', 'verbose': 0})\n",
        "}\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "all_model_results = {}\n",
        "\n",
        "for model_name, (model_class, model_params) in models_to_test.items():\n",
        "    print(f\"\\n{model_name}\")\n",
        "\n",
        "    all_fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(tqdm(cv_strategy.split(X_aug, y_aug), total=5, desc=f\"{model_name}\")):\n",
        "        X_train, X_val = X_aug.iloc[train_idx], X_aug.iloc[val_idx]\n",
        "        y_train, y_val = y_aug.iloc[train_idx], y_aug.iloc[val_idx]\n",
        "\n",
        "        dynamic_scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "        current_model_params = model_params.copy()\n",
        "\n",
        "        if model_name in [\"XGBoost\"]:\n",
        "                current_model_params['scale_pos_weight'] = dynamic_scale_pos_weight\n",
        "\n",
        "        current_model = model_class(**current_model_params)\n",
        "\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('classifier', current_model)\n",
        "        ])\n",
        "\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "        fscores = np.divide(2 * precisions * recalls, precisions + recalls,\n",
        "                            out=np.zeros_like(precisions), where=(precisions + recalls) != 0)\n",
        "\n",
        "        ix = np.argmax(fscores)\n",
        "        best_threshold = thresholds[ix] if ix < len(thresholds) else 1.0\n",
        "        y_pred_class_optimal = (y_pred_proba >= best_threshold).astype(int)\n",
        "\n",
        "        fold_scores = {\n",
        "            'PR AUC': average_precision_score(y_val, y_pred_proba),\n",
        "            'F1-Score': f1_score(y_val, y_pred_class_optimal, zero_division=0),\n",
        "            'Precision': precision_score(y_val, y_pred_class_optimal, zero_division=0),\n",
        "            'Recall': recall_score(y_val, y_pred_class_optimal, zero_division=0),\n",
        "            'Optimal Threshold': best_threshold\n",
        "        }\n",
        "        all_fold_results.append(fold_scores)\n",
        "\n",
        "    results_df = pd.DataFrame(all_fold_results)\n",
        "    mean_scores = results_df.mean()\n",
        "    all_model_results[model_name] = mean_scores\n",
        "\n",
        "for model_name, mean_scores in all_model_results.items():\n",
        "    print(f\"\\n{model_name}\")\n",
        "    print(mean_scores.to_string())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1shbGdFlxa6Cuf34eua2DbmYkwRnd1Q0S",
      "authorship_tag": "ABX9TyMOPgwlKl9LBUeNPggOvQSE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}